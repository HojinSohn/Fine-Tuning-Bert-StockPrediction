{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "773c904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date                                               News\n",
      "0      2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
      "1      2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
      "2      2016-07-01  The president of France says if Brexit won, so...\n",
      "3      2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
      "4      2016-07-01  100+ Nobel laureates urge Greenpeace to stop o...\n",
      "...           ...                                                ...\n",
      "73603  2008-06-08  b'Man goes berzerk in Akihabara and stabs ever...\n",
      "73604  2008-06-08  b'Threat of world AIDS pandemic among heterose...\n",
      "73605  2008-06-08  b'Angst in Ankara: Turkey Steers into a Danger...\n",
      "73606  2008-06-08  b\"UK: Identity cards 'could be used to spy on ...\n",
      "73607  2008-06-08  b'Marriage, they said, was reduced to the stat...\n",
      "\n",
      "[73608 rows x 2 columns]\n",
      "Index(['Date', 'News'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sohn31/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sohn31/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sohn31/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/sohn31/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download required datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    match = re.search(r\"b'(.*?)'\", text)\n",
    "\n",
    "    if match:\n",
    "        extracted_text = match.group(1)\n",
    "        text = extracted_text \n",
    "        \n",
    "    match = re.search(r'b\"(.*?)\"', text)\n",
    "\n",
    "    if match:\n",
    "        extracted_text = match.group(1)\n",
    "        text = extracted_text \n",
    "        \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and lemmatize\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "df = pd.read_csv(\"RedditNews.csv\")\n",
    "\n",
    "print(df)\n",
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6802ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date                                               News  \\\n",
      "0      2016-07-01  A 117-year-old woman in Mexico City finally re...   \n",
      "1      2016-07-01   IMF chief backs Athens as permanent Olympic host   \n",
      "2      2016-07-01  The president of France says if Brexit won, so...   \n",
      "3      2016-07-01  British Man Who Must Give Police 24 Hours' Not...   \n",
      "4      2016-07-01  100+ Nobel laureates urge Greenpeace to stop o...   \n",
      "...           ...                                                ...   \n",
      "73603  2008-06-08  b'Man goes berzerk in Akihabara and stabs ever...   \n",
      "73604  2008-06-08  b'Threat of world AIDS pandemic among heterose...   \n",
      "73605  2008-06-08  b'Angst in Ankara: Turkey Steers into a Danger...   \n",
      "73606  2008-06-08  b\"UK: Identity cards 'could be used to spy on ...   \n",
      "73607  2008-06-08  b'Marriage, they said, was reduced to the stat...   \n",
      "\n",
      "                                            cleaned_text  \\\n",
      "0      [yearold, woman, mexico, city, finally, receiv...   \n",
      "1      [imf, chief, back, athens, permanent, olympic,...   \n",
      "2        [president, france, say, brexit, donald, trump]   \n",
      "3      [british, man, must, give, police, hour, notic...   \n",
      "4      [nobel, laureate, urge, greenpeace, stop, oppo...   \n",
      "...                                                  ...   \n",
      "73603  [man, go, berzerk, akihabara, stab, everyone, ...   \n",
      "73604  [threat, world, aid, pandemic, among, heterose...   \n",
      "73605  [angst, ankara, turkey, steer, dangerous, iden...   \n",
      "73606  [uk, identity, card, could, used, spy, people,...   \n",
      "73607  [marriage, said, reduced, status, commercial, ...   \n",
      "\n",
      "                                                  tokens  \n",
      "0      [yearold, woman, mexico, city, finally, receiv...  \n",
      "1      [imf, chief, back, athens, permanent, olympic,...  \n",
      "2        [president, france, say, brexit, donald, trump]  \n",
      "3      [british, man, must, give, police, hour, notic...  \n",
      "4      [nobel, laureate, urge, greenpeace, stop, oppo...  \n",
      "...                                                  ...  \n",
      "73603  [man, go, berzerk, akihabara, stab, everyone, ...  \n",
      "73604  [threat, world, aid, pandemic, among, heterose...  \n",
      "73605  [angst, ankara, turkey, steer, dangerous, iden...  \n",
      "73606  [uk, identity, card, could, used, spy, people,...  \n",
      "73607  [marriage, said, reduced, status, commercial, ...  \n",
      "\n",
      "[73608 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df['tokens'] = df['News'].apply(clean_text)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd6fd573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['cleaned_text'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNews\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/SentimentAnalysis/lib/python3.11/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5589\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/SentimentAnalysis/lib/python3.11/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/SentimentAnalysis/lib/python3.11/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/SentimentAnalysis/lib/python3.11/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['cleaned_text'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df.drop('cleaned_text', axis= 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fed02df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('News', axis= 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d76dd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date                                             tokens\n",
      "0      2016-07-01  [yearold, woman, mexico, city, finally, receiv...\n",
      "1      2016-07-01  [imf, chief, back, athens, permanent, olympic,...\n",
      "2      2016-07-01    [president, france, say, brexit, donald, trump]\n",
      "3      2016-07-01  [british, man, must, give, police, hour, notic...\n",
      "4      2016-07-01  [nobel, laureate, urge, greenpeace, stop, oppo...\n",
      "...           ...                                                ...\n",
      "73603  2008-06-08  [man, go, berzerk, akihabara, stab, everyone, ...\n",
      "73604  2008-06-08  [threat, world, aid, pandemic, among, heterose...\n",
      "73605  2008-06-08  [angst, ankara, turkey, steer, dangerous, iden...\n",
      "73606  2008-06-08  [uk, identity, card, could, used, spy, people,...\n",
      "73607  2008-06-08  [marriage, said, reduced, status, commercial, ...\n",
      "\n",
      "[73608 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7c7f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('reddit_news_tokens.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9311cfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date         Close          High           Low          Open  \\\n",
      "0     2008-06-09  12280.320312  12331.860352  12195.320312  12210.129883   \n",
      "1     2008-06-10  12289.759766  12369.230469  12206.959961  12277.709961   \n",
      "2     2008-06-11  12083.769531  12286.669922  12079.129883  12286.339844   \n",
      "3     2008-06-12  12141.580078  12269.240234  12076.929688  12089.629883   \n",
      "4     2008-06-13  12307.349609  12310.280273  12144.589844  12144.589844   \n",
      "...          ...           ...           ...           ...           ...   \n",
      "2026  2016-06-24  17400.750000  17946.630859  17356.339844  17946.630859   \n",
      "2027  2016-06-27  17140.240234  17355.210938  17063.080078  17355.210938   \n",
      "2028  2016-06-28  17409.720703  17409.720703  17190.509766  17190.509766   \n",
      "2029  2016-06-29  17694.679688  17704.509766  17456.019531  17456.019531   \n",
      "2030  2016-06-30  17929.990234  17930.609375  17711.800781  17712.759766   \n",
      "\n",
      "         Volume  \n",
      "0     266350000  \n",
      "1     240760000  \n",
      "2     247120000  \n",
      "3     260960000  \n",
      "4     247980000  \n",
      "...         ...  \n",
      "2026  239000000  \n",
      "2027  138740000  \n",
      "2028  112190000  \n",
      "2029  106380000  \n",
      "2030  133030000  \n",
      "\n",
      "[2031 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "dji_stock_price_df = pd.read_csv(\"DJI_stock_data.csv\")\n",
    "print(dji_stock_price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1761b68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date         Close          High           Low          Open  \\\n",
      "0     2008-06-09  12280.320312  12331.860352  12195.320312  12210.129883   \n",
      "1     2008-06-10  12289.759766  12369.230469  12206.959961  12277.709961   \n",
      "2     2008-06-11  12083.769531  12286.669922  12079.129883  12286.339844   \n",
      "3     2008-06-12  12141.580078  12269.240234  12076.929688  12089.629883   \n",
      "4     2008-06-13  12307.349609  12310.280273  12144.589844  12144.589844   \n",
      "...          ...           ...           ...           ...           ...   \n",
      "2026  2016-06-24  17400.750000  17946.630859  17356.339844  17946.630859   \n",
      "2027  2016-06-27  17140.240234  17355.210938  17063.080078  17355.210938   \n",
      "2028  2016-06-28  17409.720703  17409.720703  17190.509766  17190.509766   \n",
      "2029  2016-06-29  17694.679688  17704.509766  17456.019531  17456.019531   \n",
      "2030  2016-06-30  17929.990234  17930.609375  17711.800781  17712.759766   \n",
      "\n",
      "         Volume  Close_diff   High_diff    Low_diff   Open_diff  \\\n",
      "0     266350000    9.439453   37.370117   11.639648   67.580078   \n",
      "1     240760000 -205.990234  -82.560547 -127.830078    8.629883   \n",
      "2     247120000   57.810547  -17.429688   -2.200195 -196.709961   \n",
      "3     260960000  165.769531   41.040039   67.660156   54.959961   \n",
      "4     247980000  -38.269531    9.439453   67.660156  162.270508   \n",
      "...         ...         ...         ...         ...         ...   \n",
      "2026  239000000 -260.509766 -591.419922 -293.259766 -591.419922   \n",
      "2027  138740000  269.480469   54.509766  127.429688 -164.701172   \n",
      "2028  112190000  284.958984  294.789062  265.509766  265.509766   \n",
      "2029  106380000  235.310547  226.099609  255.781250  256.740234   \n",
      "2030  133030000         NaN         NaN         NaN         NaN   \n",
      "\n",
      "      Open_Close_diff  \n",
      "0           70.190430  \n",
      "1           12.049805  \n",
      "2         -202.570312  \n",
      "3           51.950195  \n",
      "4          162.759766  \n",
      "...               ...  \n",
      "2026      -545.880859  \n",
      "2027      -214.970703  \n",
      "2028       219.210938  \n",
      "2029       238.660156  \n",
      "2030       217.230469  \n",
      "\n",
      "[2031 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "dji_stock_price_df['Close_diff'] = dji_stock_price_df['Close'].shift(-1) - dji_stock_price_df['Close']\n",
    "dji_stock_price_df['High_diff'] = dji_stock_price_df['High'].shift(-1) - dji_stock_price_df['High']\n",
    "dji_stock_price_df['Low_diff'] = dji_stock_price_df['Low'].shift(-1) - dji_stock_price_df['Low']\n",
    "dji_stock_price_df['Open_diff'] = dji_stock_price_df['Open'].shift(-1) - dji_stock_price_df['Open']\n",
    "dji_stock_price_df['Open_Close_diff'] = dji_stock_price_df['Close'] - dji_stock_price_df['Open']\n",
    "\n",
    "print(dji_stock_price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fa02952",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_news_df = pd.read_csv(\"reddit_news_tokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09ac3edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date                                             tokens\n",
      "0      2016-07-01  ['yearold', 'woman', 'mexico', 'city', 'finall...\n",
      "1      2016-07-01  ['imf', 'chief', 'back', 'athens', 'permanent'...\n",
      "2      2016-07-01  ['president', 'france', 'say', 'brexit', 'dona...\n",
      "3      2016-07-01  ['british', 'man', 'must', 'give', 'police', '...\n",
      "4      2016-07-01  ['nobel', 'laureate', 'urge', 'greenpeace', 's...\n",
      "...           ...                                                ...\n",
      "73603  2008-06-08  ['man', 'go', 'berzerk', 'akihabara', 'stab', ...\n",
      "73604  2008-06-08  ['threat', 'world', 'aid', 'pandemic', 'among'...\n",
      "73605  2008-06-08  ['angst', 'ankara', 'turkey', 'steer', 'danger...\n",
      "73606  2008-06-08  ['uk', 'identity', 'card', 'could', 'used', 's...\n",
      "73607  2008-06-08  ['marriage', 'said', 'reduced', 'status', 'com...\n",
      "\n",
      "[73608 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(reddit_news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33686cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.merge(reddit_news_df, dji_stock_price_df, on='Date', how='inner')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2e6d649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date                                             tokens  \\\n",
      "0      2016-06-30  ['jamaica', 'proposes', 'marijuana', 'dispense...   \n",
      "1      2016-06-30  ['stephen', 'hawking', 'say', 'pollution', 'st...   \n",
      "2      2016-06-30  ['boris', 'johnson', 'say', 'run', 'tory', 'pa...   \n",
      "3      2016-06-30  ['six', 'gay', 'men', 'ivory', 'coast', 'abuse...   \n",
      "4      2016-06-30  ['switzerland', 'denies', 'citizenship', 'musl...   \n",
      "...           ...                                                ...   \n",
      "50763  2008-06-09  ['future', 'united', 'state', 'europe', 'hand'...   \n",
      "50764  2008-06-09  ['military', 'coup', 'zimbabwe', 'mugabe', 'fo...   \n",
      "50765  2008-06-09  ['rising', 'oil', 'price', 'spark', 'strike', ...   \n",
      "50766  2008-06-09  ['chvez', 'farc', 'asks', 'end', 'armed', 'str...   \n",
      "50767  2008-06-09               ['flier', 'pain', 'airline', 'pack']   \n",
      "\n",
      "              Close          High           Low          Open     Volume  \\\n",
      "0      17929.990234  17930.609375  17711.800781  17712.759766  133030000   \n",
      "1      17929.990234  17930.609375  17711.800781  17712.759766  133030000   \n",
      "2      17929.990234  17930.609375  17711.800781  17712.759766  133030000   \n",
      "3      17929.990234  17930.609375  17711.800781  17712.759766  133030000   \n",
      "4      17929.990234  17930.609375  17711.800781  17712.759766  133030000   \n",
      "...             ...           ...           ...           ...        ...   \n",
      "50763  12280.320312  12331.860352  12195.320312  12210.129883  266350000   \n",
      "50764  12280.320312  12331.860352  12195.320312  12210.129883  266350000   \n",
      "50765  12280.320312  12331.860352  12195.320312  12210.129883  266350000   \n",
      "50766  12280.320312  12331.860352  12195.320312  12210.129883  266350000   \n",
      "50767  12280.320312  12331.860352  12195.320312  12210.129883  266350000   \n",
      "\n",
      "       Close_diff  High_diff   Low_diff  Open_diff  Open_Close_diff  \n",
      "0             NaN        NaN        NaN        NaN       217.230469  \n",
      "1             NaN        NaN        NaN        NaN       217.230469  \n",
      "2             NaN        NaN        NaN        NaN       217.230469  \n",
      "3             NaN        NaN        NaN        NaN       217.230469  \n",
      "4             NaN        NaN        NaN        NaN       217.230469  \n",
      "...           ...        ...        ...        ...              ...  \n",
      "50763    9.439453  37.370117  11.639648  67.580078        70.190430  \n",
      "50764    9.439453  37.370117  11.639648  67.580078        70.190430  \n",
      "50765    9.439453  37.370117  11.639648  67.580078        70.190430  \n",
      "50766    9.439453  37.370117  11.639648  67.580078        70.190430  \n",
      "50767    9.439453  37.370117  11.639648  67.580078        70.190430  \n",
      "\n",
      "[50768 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print (data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8abd977",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop(['Close', 'High', 'Low', 'Open', 'Volume'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4c74ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date                                             tokens  \\\n",
      "0      2016-06-30  ['jamaica', 'proposes', 'marijuana', 'dispense...   \n",
      "1      2016-06-30  ['stephen', 'hawking', 'say', 'pollution', 'st...   \n",
      "2      2016-06-30  ['boris', 'johnson', 'say', 'run', 'tory', 'pa...   \n",
      "3      2016-06-30  ['six', 'gay', 'men', 'ivory', 'coast', 'abuse...   \n",
      "4      2016-06-30  ['switzerland', 'denies', 'citizenship', 'musl...   \n",
      "...           ...                                                ...   \n",
      "50763  2008-06-09  ['future', 'united', 'state', 'europe', 'hand'...   \n",
      "50764  2008-06-09  ['military', 'coup', 'zimbabwe', 'mugabe', 'fo...   \n",
      "50765  2008-06-09  ['rising', 'oil', 'price', 'spark', 'strike', ...   \n",
      "50766  2008-06-09  ['chvez', 'farc', 'asks', 'end', 'armed', 'str...   \n",
      "50767  2008-06-09               ['flier', 'pain', 'airline', 'pack']   \n",
      "\n",
      "       Close_diff  High_diff   Low_diff  Open_diff  Open_Close_diff  \n",
      "0             NaN        NaN        NaN        NaN       217.230469  \n",
      "1             NaN        NaN        NaN        NaN       217.230469  \n",
      "2             NaN        NaN        NaN        NaN       217.230469  \n",
      "3             NaN        NaN        NaN        NaN       217.230469  \n",
      "4             NaN        NaN        NaN        NaN       217.230469  \n",
      "...           ...        ...        ...        ...              ...  \n",
      "50763    9.439453  37.370117  11.639648  67.580078        70.190430  \n",
      "50764    9.439453  37.370117  11.639648  67.580078        70.190430  \n",
      "50765    9.439453  37.370117  11.639648  67.580078        70.190430  \n",
      "50766    9.439453  37.370117  11.639648  67.580078        70.190430  \n",
      "50767    9.439453  37.370117  11.639648  67.580078        70.190430  \n",
      "\n",
      "[50768 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44e4d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('train_raw_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659839ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reddit_news_df = pd.read_csv(\"reddit_news_tokens.csv\")\n",
    "max_len = max(len(tokens) for tokens in reddit_news_df['tokens'])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340da2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SentimentAnalysis",
   "language": "python",
   "name": "sentimentanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
